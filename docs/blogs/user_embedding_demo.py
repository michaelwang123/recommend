import torch
import torch.nn as nn
from sklearn.preprocessing import LabelEncoder

print("=== ç”¨æˆ·è½¬æ¢ä¸º64ç»´å‘é‡è¯¦ç»†æ¼”ç¤º ===")

# æ­¥éª¤1ï¼šåŸå§‹ç”¨æˆ·æ•°æ®
print("\n1. åŸå§‹ç”¨æˆ·æ•°æ®:")
users = ["å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­", "é’±ä¸ƒ"]
print(f"ç”¨æˆ·åˆ—è¡¨: {users}")

# æ­¥éª¤2ï¼šç”¨æˆ·IDç¼–ç 
print("\n2. ç”¨æˆ·IDç¼–ç :")
user_encoder = LabelEncoder()
user_encoded = user_encoder.fit_transform(users)
print(f"ç¼–ç åçš„ç”¨æˆ·ID: {user_encoded}")
print(f"ç¼–ç æ˜ å°„å…³ç³»:")
for i, user in enumerate(users):
    print(f"  '{user}' -> {user_encoded[i]}")

# æ­¥éª¤3ï¼šåˆ›å»ºåµŒå…¥å±‚
print("\n3. åˆ›å»ºåµŒå…¥å±‚:")
n_users = len(users)  # 5ä¸ªç”¨æˆ·
embedding_dim = 64    # 64ç»´å‘é‡
user_embedding = nn.Embedding(n_users, embedding_dim)
print(f"åµŒå…¥å±‚å‚æ•°: nn.Embedding({n_users}, {embedding_dim})")
print(f"åµŒå…¥å±‚æƒé‡å½¢çŠ¶: {user_embedding.weight.shape}")
print(f"æ€»å‚æ•°æ•°é‡: {n_users * embedding_dim}")

# æ­¥éª¤4ï¼šè½¬æ¢è¿‡ç¨‹æ¼”ç¤º
print("\n4. è½¬æ¢è¿‡ç¨‹æ¼”ç¤º:")
print(f"ç”¨æˆ·'å¼ ä¸‰'çš„è½¬æ¢è¿‡ç¨‹:")
print(f"  åŸå§‹ç”¨æˆ·å: 'å¼ ä¸‰'")
print(f"  ç¼–ç åID: {user_encoded[0]}")

# è½¬æ¢ä¸ºPyTorchå¼ é‡
user_id_tensor = torch.LongTensor([user_encoded[0]])
print(f"  PyTorchå¼ é‡: {user_id_tensor}")

# é€šè¿‡åµŒå…¥å±‚è½¬æ¢
user_vector = user_embedding(user_id_tensor)
print(f"  64ç»´å‘é‡å½¢çŠ¶: {user_vector.shape}")
print(f"  64ç»´å‘é‡å‰10ä¸ªå€¼: {user_vector[0][:10].detach().numpy()}")

# æ­¥éª¤5ï¼šæ‰¹é‡è½¬æ¢
print("\n5. æ‰¹é‡è½¬æ¢æ¼”ç¤º:")
all_user_ids = torch.LongTensor(user_encoded)
all_user_vectors = user_embedding(all_user_ids)
print(f"æ‰¹é‡è¾“å…¥å½¢çŠ¶: {all_user_ids.shape}")
print(f"æ‰¹é‡è¾“å‡ºå½¢çŠ¶: {all_user_vectors.shape}")

# å±•ç¤ºæ¯ä¸ªç”¨æˆ·çš„å‘é‡
for i, user in enumerate(users):
    vector = all_user_vectors[i]
    print(f"ç”¨æˆ·'{user}': å‘é‡é•¿åº¦={len(vector)}, å‰5ä¸ªå€¼={vector[:5].detach().numpy()}")

# æ­¥éª¤6ï¼šå‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
print("\n6. å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰:")
user1_vector = all_user_vectors[0]  # å¼ ä¸‰
user2_vector = all_user_vectors[1]  # æå››
similarity = torch.cosine_similarity(user1_vector, user2_vector, dim=0)
print(f"å¼ ä¸‰å’Œæå››çš„ç›¸ä¼¼åº¦: {similarity.item():.4f}")

print("âš ï¸  WARNING: è¿™ä¸ªç›¸ä¼¼åº¦æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼å› ä¸ºå‘é‡æ˜¯éšæœºåˆå§‹åŒ–çš„")

# æ­¥éª¤7ï¼šé—®é¢˜åˆ†æ
print("\n7. é—®é¢˜åˆ†æ:")
print("âŒ å½“å‰é—®é¢˜ï¼š")
print("  â€¢ åµŒå…¥å‘é‡æ˜¯éšæœºåˆå§‹åŒ–çš„")
print("  â€¢ æ²¡æœ‰åæ˜ ç”¨æˆ·çœŸå®åå¥½")
print("  â€¢ ç›¸ä¼¼åº¦è®¡ç®—æ¯«æ— æ„ä¹‰")
print("  â€¢ æ— æ³•ç”¨äºå®é™…æ¨è")

# æ­¥éª¤8ï¼šç®€å•çš„è®­ç»ƒç¤ºä¾‹
print("\n8. ç®€å•çš„è®­ç»ƒç¤ºä¾‹:")
print("ä¸ºäº†è®©åµŒå…¥å‘é‡æœ‰æ„ä¹‰ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒæ•°æ®ï¼š")

# æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºæ•°æ®
print("\næ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºæ•°æ®:")
# å‡è®¾ï¼šå¼ ä¸‰å’Œæå››å–œæ¬¢ç§‘æŠ€äº§å“ï¼Œç‹äº”å’Œèµµå…­å–œæ¬¢æ—¶å°šäº§å“ï¼Œé’±ä¸ƒå–œæ¬¢è¿åŠ¨äº§å“
user_behavior = {
    "å¼ ä¸‰": [0, 1, 2],  # å–œæ¬¢ç‰©å“0,1,2ï¼ˆç§‘æŠ€äº§å“ï¼‰
    "æå››": [0, 1, 3],  # å–œæ¬¢ç‰©å“0,1,3ï¼ˆç§‘æŠ€äº§å“ï¼‰
    "ç‹äº”": [4, 5, 6],  # å–œæ¬¢ç‰©å“4,5,6ï¼ˆæ—¶å°šäº§å“ï¼‰
    "èµµå…­": [4, 5, 7],  # å–œæ¬¢ç‰©å“4,5,7ï¼ˆæ—¶å°šäº§å“ï¼‰
    "é’±ä¸ƒ": [8, 9, 10]  # å–œæ¬¢ç‰©å“8,9,10ï¼ˆè¿åŠ¨äº§å“ï¼‰
}

for user, items in user_behavior.items():
    print(f"  {user}: å–œæ¬¢ç‰©å“ {items}")

# åˆ›å»ºç®€å•çš„æ¨èæ¨¡å‹
print("\nåˆ›å»ºç®€å•æ¨èæ¨¡å‹:")
class SimpleRecommender(nn.Module):
    def __init__(self, n_users, n_items, embedding_dim=8):
        super().__init__()
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)
        
    def forward(self, user_ids, item_ids):
        user_emb = self.user_embedding(user_ids)
        item_emb = self.item_embedding(item_ids)
        return torch.sum(user_emb * item_emb, dim=1)

# å‡†å¤‡è®­ç»ƒæ•°æ®
train_data = []
for user, items in user_behavior.items():
    user_id = user_encoder.transform([user])[0]
    for item_id in items:
        train_data.append((user_id, item_id, 1.0))  # æ­£æ ·æœ¬

# æ·»åŠ è´Ÿæ ·æœ¬ï¼ˆç”¨æˆ·æ²¡æœ‰äº¤äº’çš„ç‰©å“ï¼‰
for user, items in user_behavior.items():
    user_id = user_encoder.transform([user])[0]
    all_items = set(range(11))  # ç‰©å“0-10
    negative_items = all_items - set(items)
    for item_id in list(negative_items)[:2]:  # åªå–2ä¸ªè´Ÿæ ·æœ¬
        train_data.append((user_id, item_id, 0.0))  # è´Ÿæ ·æœ¬

# è®­ç»ƒæ¨¡å‹
print(f"è®­ç»ƒæ•°æ®æ ·æœ¬æ•°: {len(train_data)}")
model = SimpleRecommender(n_users=5, n_items=11, embedding_dim=8)
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
criterion = nn.MSELoss()

print("\nå¼€å§‹è®­ç»ƒ...")
for epoch in range(200):
    total_loss = 0
    for user_id, item_id, rating in train_data:
        optimizer.zero_grad()
        pred = model(torch.LongTensor([user_id]), torch.LongTensor([item_id]))
        loss = criterion(pred, torch.FloatTensor([rating]))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss/len(train_data):.4f}")

# æ­¥éª¤9ï¼šè®­ç»ƒåçš„ç”¨æˆ·åµŒå…¥å‘é‡åˆ†æ
print("\n9. è®­ç»ƒåçš„ç”¨æˆ·åµŒå…¥å‘é‡åˆ†æ:")
trained_user_embeddings = model.user_embedding.weight.detach()

print("è®­ç»ƒåçš„ç”¨æˆ·åµŒå…¥å‘é‡:")
for i, user in enumerate(users):
    vector = trained_user_embeddings[i]
    print(f"ç”¨æˆ·'{user}': {vector.numpy()}")

# è®¡ç®—è®­ç»ƒåçš„ç”¨æˆ·ç›¸ä¼¼åº¦
print("\nè®­ç»ƒåçš„ç”¨æˆ·ç›¸ä¼¼åº¦:")
for i, user1 in enumerate(users):
    for j, user2 in enumerate(users):
        if i < j:
            vec1 = trained_user_embeddings[i]
            vec2 = trained_user_embeddings[j]
            similarity = torch.cosine_similarity(vec1, vec2, dim=0)
            print(f"{user1} vs {user2}: {similarity.item():.4f}")

print("\n=== å…³é”®æ´å¯Ÿ ===")
print("âœ… è®­ç»ƒåçš„åµŒå…¥å‘é‡æ‰æœ‰æ„ä¹‰ï¼š")
print("  â€¢ ç›¸ä¼¼åå¥½çš„ç”¨æˆ·åµŒå…¥å‘é‡æ›´æ¥è¿‘")
print("  â€¢ å¼ ä¸‰å’Œæå››ï¼ˆéƒ½å–œæ¬¢ç§‘æŠ€ï¼‰ç›¸ä¼¼åº¦æ›´é«˜")
print("  â€¢ ç‹äº”å’Œèµµå…­ï¼ˆéƒ½å–œæ¬¢æ—¶å°šï¼‰ç›¸ä¼¼åº¦æ›´é«˜")
print("  â€¢ ä¸åŒç±»å‹ç”¨æˆ·çš„ç›¸ä¼¼åº¦è¾ƒä½")

print("\nâŒ éšæœºåˆå§‹åŒ–çš„åµŒå…¥å‘é‡æ²¡æœ‰æ„ä¹‰ï¼š")
print("  â€¢ å‘é‡å€¼æ˜¯éšæœºçš„ï¼Œä¸åæ˜ çœŸå®åå¥½")
print("  â€¢ ç›¸ä¼¼åº¦è®¡ç®—ç»“æœæ¯«æ— æ„ä¹‰")
print("  â€¢ æ— æ³•ç”¨äºå®é™…æ¨èç³»ç»Ÿ")

print("\n=== è½¬æ¢è¿‡ç¨‹æ€»ç»“ ===")
print("1. ç”¨æˆ·å â†’ æ•°å­—IDï¼ˆç¼–ç ï¼‰")
print("2. æ•°å­—ID â†’ PyTorchå¼ é‡")
print("3. å¼ é‡ â†’ 64ç»´å‘é‡ï¼ˆåµŒå…¥å±‚ï¼‰")
print("4. ğŸ”¥ å…³é”®æ­¥éª¤ï¼šé€šè¿‡ç”¨æˆ·è¡Œä¸ºæ•°æ®è®­ç»ƒå‘é‡")
print("5. è®­ç»ƒåçš„å‘é‡æ‰èƒ½ç”¨äºæ¨èè¯„åˆ†")

print("\nğŸ“š å®é™…æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼š")
print("  â€¢ æ”¶é›†ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼ˆç‚¹å‡»ã€è´­ä¹°ã€è¯„åˆ†ç­‰ï¼‰")
print("  â€¢ é€šè¿‡ååŒè¿‡æ»¤æˆ–æ·±åº¦å­¦ä¹ è®­ç»ƒæ¨¡å‹")
print("  â€¢ å­¦ä¹ åˆ°çš„åµŒå…¥å‘é‡æ‰èƒ½åæ˜ ç”¨æˆ·åå¥½")
print("  â€¢ é¡¹ç›®ä¸­çš„ meaningful_user_embedding.py æä¾›äº†å®Œæ•´ç¤ºä¾‹") 