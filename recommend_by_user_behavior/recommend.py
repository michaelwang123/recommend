#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºMySQLæ•°æ®çš„æ¨èç³»ç»Ÿ
ä»MySQLä¸­è·å–éè¿ç»­ç”¨æˆ·IDå’Œç‰©å“IDæ•°æ®ï¼Œè®­ç»ƒæ¨èæ¨¡å‹
"""

import mysql.connector
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import pickle
import json
from datetime import datetime
import os

class MySQLRecommendationSystem:
    def __init__(self, db_config=None):
        """åˆå§‹åŒ–æ¨èç³»ç»Ÿ"""
        # é»˜è®¤æ•°æ®åº“é…ç½®
        self.db_config = db_config or {
            'host': 'localhost',
            'port': 3306,
            'user': 'test',
            'password': 'test',
            'database': 'testdb'
        }
        
        # æ¨¡å‹å‚æ•°
        self.embedding_dim = 64
        self.model = None
        
        # IDæ˜ å°„å­—å…¸
        self.user_id_to_idx = {}
        self.item_id_to_idx = {}
        self.idx_to_user_id = {}
        self.idx_to_item_id = {}
        
        # æ•°æ®ç»Ÿè®¡
        self.num_users = 0
        self.num_items = 0
        
        print("ğŸš€ MySQLæ¨èç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def connect_to_database(self):
        """è¿æ¥åˆ°MySQLæ•°æ®åº“"""
        try:
            print("ğŸ”Œ è¿æ¥åˆ°MySQLæ•°æ®åº“...")
            connection = mysql.connector.connect(**self.db_config)
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return connection
        except mysql.connector.Error as err:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {err}")
            return None
    
    def load_data_from_mysql(self):
        """ä»MySQLåŠ è½½ç”¨æˆ·è¡Œä¸ºæ•°æ®"""
        print("ğŸ“Š ä»MySQLåŠ è½½ç”¨æˆ·è¡Œä¸ºæ•°æ®...")
        
        connection = self.connect_to_database()
        if not connection:
            return None
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            query = """
            SELECT user_id, item_id, rating, user_type, item_category, created_at
            FROM user_behavior
            ORDER BY created_at
            """
            
            df = pd.read_sql(query, connection)
            
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
            print(f"   å”¯ä¸€ç”¨æˆ·æ•°: {df['user_id'].nunique()}")
            print(f"   å”¯ä¸€ç‰©å“æ•°: {df['item_id'].nunique()}")
            print(f"   è¯„åˆ†èŒƒå›´: {df['rating'].min():.2f} - {df['rating'].max():.2f}")
            print(f"   å¹³å‡è¯„åˆ†: {df['rating'].mean():.2f}")
            
            # æ˜¾ç¤ºæ•°æ®ç±»å‹åˆ†å¸ƒ
            print(f"\nç”¨æˆ·ç±»å‹åˆ†å¸ƒ:")
            user_type_stats = df.groupby('user_type').agg({
                'user_id': 'count',
                'rating': 'mean'
            }).round(2)
            user_type_stats.columns = ['è®°å½•æ•°', 'å¹³å‡è¯„åˆ†']
            print(user_type_stats)
            
            return df
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
        
        finally:
            connection.close()
            print("ğŸ”Œ æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def create_id_mappings(self, df):
        """åˆ›å»ºIDæ˜ å°„ï¼ˆéè¿ç»­ID â†’ è¿ç»­ç´¢å¼•ï¼‰"""
        print("\nğŸ”„ åˆ›å»ºIDæ˜ å°„...")
        
        # è·å–å”¯ä¸€IDå¹¶æ’åº
        unique_users = sorted(df['user_id'].unique())
        unique_items = sorted(df['item_id'].unique())
        
        # åˆ›å»ºæ˜ å°„å­—å…¸
        self.user_id_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}
        self.item_id_to_idx = {item_id: idx for idx, item_id in enumerate(unique_items)}
        
        # åˆ›å»ºåå‘æ˜ å°„
        self.idx_to_user_id = {idx: user_id for user_id, idx in self.user_id_to_idx.items()}
        self.idx_to_item_id = {idx: item_id for item_id, idx in self.item_id_to_idx.items()}
        
        # æ›´æ–°æ•°é‡
        self.num_users = len(unique_users)
        self.num_items = len(unique_items)
        
        print(f"âœ… IDæ˜ å°„åˆ›å»ºå®Œæˆ:")
        print(f"   ç”¨æˆ·æ˜ å°„: {self.num_users} ä¸ªç”¨æˆ·")
        print(f"   ç‰©å“æ˜ å°„: {self.num_items} ä¸ªç‰©å“")
        
        # æ˜¾ç¤ºæ˜ å°„ç¤ºä¾‹
        print(f"\nğŸ“ æ˜ å°„ç¤ºä¾‹:")
        sample_users = list(self.user_id_to_idx.items())[:5]
        sample_items = list(self.item_id_to_idx.items())[:5]
        
        for user_id, idx in sample_users:
            print(f"   ç”¨æˆ· {user_id} â†’ ç´¢å¼• {idx}")
        
        for item_id, idx in sample_items:
            print(f"   ç‰©å“ {item_id} â†’ ç´¢å¼• {idx}")
        
        return True
    
    def prepare_training_data(self, df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆè½¬æ¢ä¸ºè¿ç»­ç´¢å¼•ï¼‰"""
        print("\nğŸ“‹ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # è½¬æ¢IDä¸ºç´¢å¼•
        df['user_idx'] = df['user_id'].map(self.user_id_to_idx)
        df['item_idx'] = df['item_id'].map(self.item_id_to_idx)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ å°„å¤±è´¥çš„æ•°æ®
        missing_users = df[df['user_idx'].isna()]
        missing_items = df[df['item_idx'].isna()]
        
        if len(missing_users) > 0:
            print(f"âš ï¸  å‘ç° {len(missing_users)} æ¡ç”¨æˆ·IDæ˜ å°„å¤±è´¥çš„è®°å½•")
        
        if len(missing_items) > 0:
            print(f"âš ï¸  å‘ç° {len(missing_items)} æ¡ç‰©å“IDæ˜ å°„å¤±è´¥çš„è®°å½•")
        
        # åˆ é™¤æ˜ å°„å¤±è´¥çš„è®°å½•
        df_clean = df.dropna(subset=['user_idx', 'item_idx'])
        
        print(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   æœ‰æ•ˆè®°å½•æ•°: {len(df_clean)}")
        print(f"   ç”¨æˆ·ç´¢å¼•èŒƒå›´: 0 - {df_clean['user_idx'].max()}")
        print(f"   ç‰©å“ç´¢å¼•èŒƒå›´: 0 - {df_clean['item_idx'].max()}")
        
        return df_clean
    
    def create_recommendation_model(self):
        """åˆ›å»ºæ¨èæ¨¡å‹ï¼ˆçŸ©é˜µåˆ†è§£ï¼‰"""
        class MatrixFactorization(nn.Module):
            def __init__(self, num_users, num_items, embedding_dim):
                super().__init__()
                # ç”¨æˆ·åµŒå…¥å±‚
                self.user_embedding = nn.Embedding(num_users, embedding_dim)
                # ç‰©å“åµŒå…¥å±‚
                self.item_embedding = nn.Embedding(num_items, embedding_dim)
                
                # åç½®é¡¹
                self.user_bias = nn.Embedding(num_users, 1)
                self.item_bias = nn.Embedding(num_items, 1)
                self.global_bias = nn.Parameter(torch.zeros(1))
                
                # åˆå§‹åŒ–æƒé‡
                self._init_weights()
            
            def _init_weights(self):
                """åˆå§‹åŒ–æƒé‡"""
                nn.init.normal_(self.user_embedding.weight, std=0.1)
                nn.init.normal_(self.item_embedding.weight, std=0.1)
                nn.init.normal_(self.user_bias.weight, std=0.1)
                nn.init.normal_(self.item_bias.weight, std=0.1)
            
            def forward(self, user_ids, item_ids):
                # è·å–åµŒå…¥å‘é‡
                user_vectors = self.user_embedding(user_ids)
                item_vectors = self.item_embedding(item_ids)
                
                # è®¡ç®—äº¤äº’å¾—åˆ†
                interaction = torch.sum(user_vectors * item_vectors, dim=1)
                
                # æ·»åŠ åç½®é¡¹
                user_bias = self.user_bias(user_ids).squeeze()
                item_bias = self.item_bias(item_ids).squeeze()
                
                # æœ€ç»ˆé¢„æµ‹
                prediction = interaction + user_bias + item_bias + self.global_bias
                
                return prediction
        
        return MatrixFactorization(self.num_users, self.num_items, self.embedding_dim)
    
    def train_model(self, df_train, epochs=100, learning_rate=0.01, test_size=0.2):
        """è®­ç»ƒæ¨èæ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨èæ¨¡å‹...")
        
        # å‡†å¤‡å¼ é‡æ•°æ®
        user_ids = torch.tensor(df_train['user_idx'].values, dtype=torch.long)
        item_ids = torch.tensor(df_train['item_idx'].values, dtype=torch.long)
        ratings = torch.tensor(df_train['rating'].values, dtype=torch.float32)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        train_indices, val_indices = train_test_split(
            range(len(df_train)), test_size=test_size, random_state=42
        )
        
        # åˆ›å»ºæ¨¡å‹
        self.model = self.create_recommendation_model()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()
        
        print(f"æ¨¡å‹å‚æ•°:")
        print(f"  ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"  ç‰©å“æ•°é‡: {self.num_items}")
        print(f"  åµŒå…¥ç»´åº¦: {self.embedding_dim}")
        print(f"  æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  è®­ç»ƒé›†å¤§å°: {len(train_indices)}")
        print(f"  éªŒè¯é›†å¤§å°: {len(val_indices)}")
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        val_losses = []
        
        self.model.train()
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_user_ids = user_ids[train_indices]
            train_item_ids = item_ids[train_indices]
            train_ratings = ratings[train_indices]
            
            optimizer.zero_grad()
            predictions = self.model(train_user_ids, train_item_ids)
            train_loss = criterion(predictions, train_ratings)
            train_loss.backward()
            optimizer.step()
            
            train_losses.append(train_loss.item())
            
            # éªŒè¯é˜¶æ®µ
            if epoch % 10 == 0:
                self.model.eval()
                with torch.no_grad():
                    val_user_ids = user_ids[val_indices]
                    val_item_ids = item_ids[val_indices]
                    val_ratings = ratings[val_indices]
                    
                    val_predictions = self.model(val_user_ids, val_item_ids)
                    val_loss = criterion(val_predictions, val_ratings)
                    val_losses.append(val_loss.item())
                    
                    print(f"  Epoch {epoch:3d}: è®­ç»ƒæŸå¤±={train_loss.item():.4f}, éªŒè¯æŸå¤±={val_loss.item():.4f}")
                
                self.model.train()
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        # æœ€ç»ˆéªŒè¯
        self.model.eval()
        with torch.no_grad():
            val_user_ids = user_ids[val_indices]
            val_item_ids = item_ids[val_indices]
            val_ratings = ratings[val_indices]
            
            final_predictions = self.model(val_user_ids, val_item_ids)
            final_loss = criterion(final_predictions, val_ratings)
            
            print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {final_loss.item():.4f}")
        
        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'final_loss': final_loss.item()
        }
    
    def get_user_recommendations(self, user_id, top_n=10, exclude_rated=True, df=None):
        """ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆæ¨è"""
        if self.model is None:
            print("âŒ æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_model()")
            return []
        
        # æ£€æŸ¥ç”¨æˆ·IDæ˜¯å¦å­˜åœ¨
        if user_id not in self.user_id_to_idx:
            print(f"âŒ ç”¨æˆ· {user_id} ä¸å­˜åœ¨äºè®­ç»ƒæ•°æ®ä¸­")
            return []
        
        user_idx = self.user_id_to_idx[user_id]
        
        print(f"\nğŸ¯ ä¸ºç”¨æˆ· {user_id} (ç´¢å¼•:{user_idx}) ç”Ÿæˆæ¨è...")
        
        # å‡†å¤‡æ•°æ®
        user_tensor = torch.tensor([user_idx])
        all_items = torch.arange(self.num_items)
        
        # æ‰¹é‡é¢„æµ‹æ‰€æœ‰ç‰©å“çš„è¯„åˆ†
        self.model.eval()
        with torch.no_grad():
            user_repeated = user_tensor.repeat(self.num_items)
            predictions = self.model(user_repeated, all_items)
        
        # æ’é™¤å·²è¯„åˆ†çš„ç‰©å“
        available_items = list(range(self.num_items))
        
        if exclude_rated and df is not None:
            user_history = df[df['user_id'] == user_id]['item_id'].unique()
            rated_item_indices = [self.item_id_to_idx[item_id] for item_id in user_history 
                                if item_id in self.item_id_to_idx]
            available_items = [idx for idx in available_items if idx not in rated_item_indices]
            print(f"   æ’é™¤å·²è¯„åˆ†ç‰©å“: {len(rated_item_indices)} ä¸ª")
        
        # è·å–å¯æ¨èç‰©å“çš„é¢„æµ‹è¯„åˆ†
        available_predictions = predictions[available_items]
        available_item_indices = torch.tensor(available_items)
        
        # è·å–TopNæ¨è
        top_scores, top_indices = torch.topk(available_predictions, k=min(top_n, len(available_items)))
        top_item_indices = available_item_indices[top_indices]
        
        # è½¬æ¢å›åŸå§‹ç‰©å“ID
        recommendations = []
        for i, (item_idx, score) in enumerate(zip(top_item_indices, top_scores)):
            item_id = self.idx_to_item_id[item_idx.item()]
            recommendations.append({
                'rank': i + 1,
                'item_id': item_id,
                'item_idx': item_idx.item(),
                'predicted_rating': score.item()
            })
        
        print(f"âœ… ç”Ÿæˆäº† {len(recommendations)} ä¸ªæ¨è")
        
        return recommendations
    
    def analyze_user_groups(self, df):
        """åˆ†æä¸åŒç”¨æˆ·ç¾¤ä½“çš„ç‰¹å¾"""
        print(f"\nğŸ” åˆ†æç”¨æˆ·ç¾¤ä½“ç‰¹å¾...")
        
        if self.model is None:
            print("âŒ æ¨¡å‹å°šæœªè®­ç»ƒ")
            return
        
        # è·å–æ‰€æœ‰ç”¨æˆ·çš„åµŒå…¥å‘é‡
        all_user_indices = torch.arange(self.num_users)
        user_embeddings = self.model.user_embedding(all_user_indices)
        
        # æŒ‰ç”¨æˆ·ç±»å‹åˆ†ç»„åˆ†æ
        user_types = df[['user_id', 'user_type']].drop_duplicates()
        
        print(f"\nç”¨æˆ·ç¾¤ä½“ç›¸ä¼¼æ€§åˆ†æ:")
        
        for user_type in user_types['user_type'].unique():
            type_users = user_types[user_types['user_type'] == user_type]['user_id'].tolist()
            type_indices = [self.user_id_to_idx[user_id] for user_id in type_users 
                           if user_id in self.user_id_to_idx]
            
            if len(type_indices) > 1:
                type_embeddings = user_embeddings[type_indices]
                
                # è®¡ç®—ç¾¤ä½“å†…ç›¸ä¼¼åº¦
                similarities = torch.cosine_similarity(
                    type_embeddings.unsqueeze(1),
                    type_embeddings.unsqueeze(0),
                    dim=2
                )
                
                # æ’é™¤å¯¹è§’çº¿
                mask = torch.eye(len(type_indices), dtype=torch.bool)
                similarities_no_diag = similarities[~mask]
                avg_similarity = similarities_no_diag.mean().item()
                
                print(f"  {user_type}: {len(type_indices)} ä¸ªç”¨æˆ·, å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    def demonstrate_recommendations(self, df):
        """æ¼”ç¤ºæ¨èæ•ˆæœ"""
        print(f"\nğŸ¯ æ¼”ç¤ºæ¨èæ•ˆæœ...")
        
        # ä¸ºæ¯ç§ç”¨æˆ·ç±»å‹é€‰æ‹©ä¸€ä¸ªä»£è¡¨ç”¨æˆ·
        user_types = df[['user_id', 'user_type']].drop_duplicates()
        
        for user_type in user_types['user_type'].unique():
            type_users = user_types[user_types['user_type'] == user_type]['user_id'].tolist()
            if type_users:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªç”¨æˆ·ä½œä¸ºä»£è¡¨
                representative_user = type_users[0]
                
                print(f"\nğŸ‘¤ ä»£è¡¨ç”¨æˆ·: {representative_user} ({user_type})")
                
                # æ˜¾ç¤ºç”¨æˆ·å†å²
                user_history = df[df['user_id'] == representative_user]
                print(f"   å†å²è¡Œä¸º: {len(user_history)} æ¡è®°å½•")
                print(f"   å¹³å‡è¯„åˆ†: {user_history['rating'].mean():.2f}")
                print(f"   äº¤äº’ç‰©å“: {user_history['item_id'].tolist()[:5]}")
                
                # ç”Ÿæˆæ¨è
                recommendations = self.get_user_recommendations(
                    representative_user, top_n=5, exclude_rated=True, df=df
                )
                
                print(f"   æ¨èç»“æœ:")
                for rec in recommendations:
                    print(f"     {rec['rank']}. {rec['item_id']} - é¢„æµ‹è¯„åˆ†: {rec['predicted_rating']:.2f}")
    
    def save_model_and_mappings(self, save_dir="./saved_model"):
        """ä¿å­˜æ¨¡å‹å’ŒIDæ˜ å°„"""
        if self.model is None:
            print("âŒ æ²¡æœ‰æ¨¡å‹å¯ä¿å­˜")
            return False
        
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {save_dir}...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save(self.model.state_dict(), f"{save_dir}/model.pth")
        
        # ä¿å­˜æ˜ å°„å­—å…¸
        mappings = {
            'user_id_to_idx': self.user_id_to_idx,
            'item_id_to_idx': self.item_id_to_idx,
            'idx_to_user_id': self.idx_to_user_id,
            'idx_to_item_id': self.idx_to_item_id,
            'num_users': self.num_users,
            'num_items': self.num_items,
            'embedding_dim': self.embedding_dim
        }
        
        with open(f"{save_dir}/mappings.json", 'w', encoding='utf-8') as f:
            json.dump(mappings, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é…ç½®
        config = {
            'model_class': 'MatrixFactorization',
            'embedding_dim': self.embedding_dim,
            'num_users': self.num_users,
            'num_items': self.num_items,
            'save_time': datetime.now().isoformat()
        }
        
        with open(f"{save_dir}/config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ:")
        print(f"   æ¨¡å‹æ–‡ä»¶: {save_dir}/model.pth")
        print(f"   æ˜ å°„æ–‡ä»¶: {save_dir}/mappings.json")
        print(f"   é…ç½®æ–‡ä»¶: {save_dir}/config.json")
        
        return True
    
    def load_model_and_mappings(self, save_dir="./saved_model"):
        """åŠ è½½æ¨¡å‹å’ŒIDæ˜ å°„"""
        print(f"ğŸ“‚ ä» {save_dir} åŠ è½½æ¨¡å‹...")
        
        try:
            # åŠ è½½æ˜ å°„
            with open(f"{save_dir}/mappings.json", 'r', encoding='utf-8') as f:
                mappings = json.load(f)
            
            self.user_id_to_idx = mappings['user_id_to_idx']
            self.item_id_to_idx = mappings['item_id_to_idx']
            self.idx_to_user_id = {int(k): v for k, v in mappings['idx_to_user_id'].items()}
            self.idx_to_item_id = {int(k): v for k, v in mappings['idx_to_item_id'].items()}
            self.num_users = mappings['num_users']
            self.num_items = mappings['num_items']
            self.embedding_dim = mappings['embedding_dim']
            
            # åˆ›å»ºæ¨¡å‹
            self.model = self.create_recommendation_model()
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(torch.load(f"{save_dir}/model.pth"))
            self.model.eval()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ:")
            print(f"   ç”¨æˆ·æ•°é‡: {self.num_users}")
            print(f"   ç‰©å“æ•°é‡: {self.num_items}")
            print(f"   åµŒå…¥ç»´åº¦: {self.embedding_dim}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ¨èç³»ç»Ÿæµç¨‹"""
        print("ğŸš€ è¿è¡Œå®Œæ•´çš„æ¨èç³»ç»Ÿæµç¨‹")
        print("=" * 80)
        
        try:
            # 1. ä»MySQLåŠ è½½æ•°æ®
            df = self.load_data_from_mysql()
            if df is None:
                return False
            
            # 2. åˆ›å»ºIDæ˜ å°„
            if not self.create_id_mappings(df):
                return False
            
            # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
            df_train = self.prepare_training_data(df)
            if df_train is None or len(df_train) == 0:
                return False
            
            # 4. è®­ç»ƒæ¨¡å‹
            training_result = self.train_model(df_train)
            
            # 5. åˆ†æç”¨æˆ·ç¾¤ä½“
            self.analyze_user_groups(df)
            
            # 6. æ¼”ç¤ºæ¨èæ•ˆæœ
            self.demonstrate_recommendations(df)
            
            # 7. ä¿å­˜æ¨¡å‹
            self.save_model_and_mappings()
            
            print("\n" + "=" * 80)
            print("ğŸ¯ æ¨èç³»ç»Ÿè®­ç»ƒå®Œæˆ!")
            print("â€¢ æˆåŠŸä»MySQLåŠ è½½éè¿ç»­IDæ•°æ®")
            print("â€¢ åˆ›å»ºäº†å®Œæ•´çš„IDæ˜ å°„æœºåˆ¶")
            print("â€¢ è®­ç»ƒäº†çŸ©é˜µåˆ†è§£æ¨èæ¨¡å‹")
            print("â€¢ åˆ†æäº†ç”¨æˆ·ç¾¤ä½“ç‰¹å¾")
            print("â€¢ æ¼”ç¤ºäº†ä¸ªæ€§åŒ–æ¨èæ•ˆæœ")
            print("â€¢ ä¿å­˜äº†æ¨¡å‹å’Œé…ç½®æ–‡ä»¶")
            print("=" * 80)
            
            return True
            
        except Exception as e:
            print(f"âŒ æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæ¨èç³»ç»Ÿå®ä¾‹
    recommender = MySQLRecommendationSystem()
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    success = recommender.run_complete_pipeline()
    
    if success:
        print("\nâœ… æ¨èç³»ç»Ÿæ„å»ºæˆåŠŸ!")
        print("\nğŸ¯ åç»­å¯ä»¥:")
        print("1. ä½¿ç”¨ get_user_recommendations() ä¸ºç”¨æˆ·ç”Ÿæˆæ¨è")
        print("2. ä½¿ç”¨ save_model_and_mappings() ä¿å­˜æ¨¡å‹")
        print("3. ä½¿ç”¨ load_model_and_mappings() åŠ è½½æ¨¡å‹")
        print("4. é›†æˆåˆ°Web APIæœåŠ¡ä¸­")
    else:
        print("\nâŒ æ¨èç³»ç»Ÿæ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()
