#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorchç”¨æˆ·IDåµŒå…¥å‘é‡ç”Ÿæˆç¤ºä¾‹
ç®€å•å®ç”¨çš„ä»£ç ç¤ºèŒƒï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨nn.Embeddingç”Ÿæˆç”¨æˆ·åµŒå…¥å‘é‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class UserEmbeddingDemo:
    def __init__(self, num_users=1000, embedding_dim=64):
        """
        åˆå§‹åŒ–ç”¨æˆ·åµŒå…¥æ¼”ç¤º
        
        Args:
            num_users: ç”¨æˆ·æ€»æ•°
            embedding_dim: åµŒå…¥å‘é‡ç»´åº¦
        """
        self.num_users = num_users
        self.embedding_dim = embedding_dim
        
        # åˆ›å»ºç”¨æˆ·åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        
        print(f"âœ… åˆ›å»ºç”¨æˆ·åµŒå…¥å±‚ï¼š")
        print(f"   ç”¨æˆ·æ•°é‡: {num_users}")
        print(f"   åµŒå…¥ç»´åº¦: {embedding_dim}")
        print(f"   å‚æ•°æ•°é‡: {num_users * embedding_dim:,}")
        print(f"   åµŒå…¥çŸ©é˜µå½¢çŠ¶: {self.user_embedding.weight.shape}")
        
    def basic_embedding_example(self):
        """åŸºç¡€åµŒå…¥å‘é‡ç”Ÿæˆç¤ºä¾‹"""
        print(f"\nğŸ” åŸºç¡€åµŒå…¥å‘é‡ç”Ÿæˆç¤ºä¾‹")
        print("=" * 60)
        
        # å•ä¸ªç”¨æˆ·ID
        user_id = torch.tensor([123])
        user_vector = self.user_embedding(user_id)
        
        print(f"å•ä¸ªç”¨æˆ·ç¤ºä¾‹ï¼š")
        print(f"  ç”¨æˆ·ID: {user_id.item()}")
        print(f"  è¾“å…¥å½¢çŠ¶: {user_id.shape}")
        print(f"  è¾“å‡ºå½¢çŠ¶: {user_vector.shape}")
        print(f"  åµŒå…¥å‘é‡å‰5ç»´: {user_vector[0][:5].detach().numpy()}")
        
        # æ‰¹é‡ç”¨æˆ·ID
        user_ids = torch.tensor([10, 25, 50, 100, 200])
        user_vectors = self.user_embedding(user_ids)
        
        print(f"\næ‰¹é‡ç”¨æˆ·ç¤ºä¾‹ï¼š")
        print(f"  ç”¨æˆ·IDs: {user_ids.tolist()}")
        print(f"  è¾“å…¥å½¢çŠ¶: {user_ids.shape}")
        print(f"  è¾“å‡ºå½¢çŠ¶: {user_vectors.shape}")
        
        # æ˜¾ç¤ºæ¯ä¸ªç”¨æˆ·çš„åµŒå…¥å‘é‡
        for i, uid in enumerate(user_ids):
            vector = user_vectors[i]
            print(f"  ç”¨æˆ·{uid.item():3d}: {vector[:5].detach().numpy()} ...")
            
        return user_vectors
    
    def embedding_similarity_example(self):
        """åµŒå…¥å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹"""
        print(f"\nğŸ“Š åµŒå…¥å‘é‡ç›¸ä¼¼åº¦è®¡ç®—")
        print("=" * 60)
        
        # é€‰æ‹©å‡ ä¸ªç”¨æˆ·
        user_ids = torch.tensor([10, 11, 50, 100])
        user_vectors = self.user_embedding(user_ids)
        
        print(f"è®¡ç®—ç”¨æˆ·é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼š")
        print(f"ç”¨æˆ·IDs: {user_ids.tolist()}")
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        def cosine_similarity(v1, v2):
            return torch.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()
        
        print(f"\nç›¸ä¼¼åº¦çŸ©é˜µï¼š")
        print("ç”¨æˆ·ID  ", end="")
        for uid in user_ids:
            print(f"{uid.item():8d}", end="")
        print()
        
        for i, uid1 in enumerate(user_ids):
            print(f"ç”¨æˆ·{uid1.item():3d}  ", end="")
            for j, uid2 in enumerate(user_ids):
                sim = cosine_similarity(user_vectors[i], user_vectors[j])
                print(f"{sim:8.3f}", end="")
            print()
        
        return user_vectors
    
    def training_example(self):
        """è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹"""
        print(f"\nğŸš€ è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹")
        print("=" * 60)
        
        # åˆ›å»ºç®€å•çš„æ¨èæ¨¡å‹
        class SimpleRecommendModel(nn.Module):
            def __init__(self, num_users, num_items, embedding_dim):
                super().__init__()
                self.user_embedding = nn.Embedding(num_users, embedding_dim)
                self.item_embedding = nn.Embedding(num_items, embedding_dim)
                
            def forward(self, user_ids, item_ids):
                user_vectors = self.user_embedding(user_ids)
                item_vectors = self.item_embedding(item_ids)
                
                # è®¡ç®—ç”¨æˆ·å’Œç‰©å“çš„ç›¸ä¼¼åº¦å¾—åˆ†
                scores = torch.sum(user_vectors * item_vectors, dim=1)
                return scores
        
        # æ¨¡å‹å‚æ•°
        num_items = 500
        model = SimpleRecommendModel(self.num_users, num_items, self.embedding_dim)
        optimizer = optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.MSELoss()
        
        print(f"æ¨¡å‹ç»“æ„ï¼š")
        print(f"  ç”¨æˆ·æ•°: {self.num_users}, ç‰©å“æ•°: {num_items}")
        print(f"  åµŒå…¥ç»´åº¦: {self.embedding_dim}")
        print(f"  æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        batch_size = 32
        user_ids = torch.randint(0, self.num_users, (batch_size,))
        item_ids = torch.randint(0, num_items, (batch_size,))
        ratings = torch.rand(batch_size) * 5  # æ¨¡æ‹Ÿè¯„åˆ† 0-5
        
        print(f"\nè®­ç»ƒæ•°æ®ç¤ºä¾‹ï¼š")
        print(f"  æ‰¹å¤§å°: {batch_size}")
        print(f"  ç”¨æˆ·IDæ ·æœ¬: {user_ids[:5].tolist()}")
        print(f"  ç‰©å“IDæ ·æœ¬: {item_ids[:5].tolist()}")
        print(f"  è¯„åˆ†æ ·æœ¬: {ratings[:5].tolist()}")
        
        # è®­ç»ƒå‡ ä¸ªæ­¥éª¤
        print(f"\nå¼€å§‹è®­ç»ƒ...")
        initial_user_embedding = model.user_embedding.weight.data[0].clone()
        
        for epoch in range(5):
            # å‰å‘ä¼ æ’­
            predicted_scores = model(user_ids, item_ids)
            loss = criterion(predicted_scores, ratings)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            print(f"  Epoch {epoch+1}: Loss = {loss.item():.4f}")
        
        # æ£€æŸ¥å‚æ•°æ›´æ–°
        final_user_embedding = model.user_embedding.weight.data[0]
        change = torch.norm(final_user_embedding - initial_user_embedding).item()
        
        print(f"\nè®­ç»ƒç»“æœï¼š")
        print(f"  ç”¨æˆ·0åµŒå…¥å‘é‡å˜åŒ–é‡: {change:.6f}")
        print(f"  è®­ç»ƒå‰: {initial_user_embedding[:5].numpy()}")
        print(f"  è®­ç»ƒå: {final_user_embedding[:5].numpy()}")
        
        return model
    
    def practical_application_example(self):
        """å®é™…åº”ç”¨ç¤ºä¾‹"""
        print(f"\nğŸ¯ å®é™…åº”ç”¨ç¤ºä¾‹")
        print("=" * 60)
        
        # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºæ•°æ®
        np.random.seed(42)
        
        # ç”Ÿæˆç”¨æˆ·äº¤äº’æ•°æ®
        num_interactions = 10000
        user_ids = np.random.randint(0, self.num_users, num_interactions)
        item_ids = np.random.randint(0, 100, num_interactions)  # 100ä¸ªç‰©å“
        ratings = np.random.normal(3.5, 1.0, num_interactions)  # æ­£æ€åˆ†å¸ƒè¯„åˆ†
        ratings = np.clip(ratings, 1, 5)  # é™åˆ¶åœ¨1-5èŒƒå›´
        
        # è½¬æ¢ä¸ºDataFrame
        interactions_df = pd.DataFrame({
            'user_id': user_ids,
            'item_id': item_ids,
            'rating': ratings
        })
        
        print(f"ç”¨æˆ·äº¤äº’æ•°æ®ç»Ÿè®¡ï¼š")
        print(f"  äº¤äº’æ€»æ•°: {len(interactions_df):,}")
        print(f"  ç”¨æˆ·æ•°: {interactions_df['user_id'].nunique()}")
        print(f"  ç‰©å“æ•°: {interactions_df['item_id'].nunique()}")
        print(f"  å¹³å‡è¯„åˆ†: {interactions_df['rating'].mean():.2f}")
        
        # å±•ç¤ºæ•°æ®æ ·æœ¬
        print(f"\næ•°æ®æ ·æœ¬ï¼š")
        print(interactions_df.head(10))
        
        # è·å–ç‰¹å®šç”¨æˆ·çš„åµŒå…¥å‘é‡
        target_user_id = 42
        user_vector = self.user_embedding(torch.tensor([target_user_id]))
        
        print(f"\nç”¨æˆ·{target_user_id}çš„åµŒå…¥å‘é‡ï¼š")
        print(f"  å‘é‡ç»´åº¦: {user_vector.shape}")
        print(f"  å‘é‡é¢„è§ˆ: {user_vector[0][:10].detach().numpy()}")
        
        # è®¡ç®—è¯¥ç”¨æˆ·ä¸å…¶ä»–ç”¨æˆ·çš„ç›¸ä¼¼åº¦
        sample_users = torch.tensor([10, 20, 30, 40, 50])
        sample_vectors = self.user_embedding(sample_users)
        
        similarities = torch.cosine_similarity(
            user_vector.expand_as(sample_vectors), 
            sample_vectors
        )
        
        print(f"\nç”¨æˆ·{target_user_id}ä¸å…¶ä»–ç”¨æˆ·çš„ç›¸ä¼¼åº¦ï¼š")
        for i, uid in enumerate(sample_users):
            print(f"  ç”¨æˆ·{uid.item():2d}: {similarities[i].item():.4f}")
        
        return interactions_df, user_vector
    
    def save_and_load_example(self):
        """ä¿å­˜å’ŒåŠ è½½åµŒå…¥å‘é‡ç¤ºä¾‹"""
        print(f"\nğŸ’¾ ä¿å­˜å’ŒåŠ è½½åµŒå…¥å‘é‡")
        print("=" * 60)
        
        # ä¿å­˜åµŒå…¥å‘é‡
        torch.save(self.user_embedding.state_dict(), 'user_embedding.pth')
        print(f"âœ… åµŒå…¥å‘é‡å·²ä¿å­˜åˆ° 'user_embedding.pth'")
        
        # åˆ›å»ºæ–°çš„åµŒå…¥å±‚å¹¶åŠ è½½å‚æ•°
        new_embedding = nn.Embedding(self.num_users, self.embedding_dim)
        new_embedding.load_state_dict(torch.load('user_embedding.pth'))
        
        print(f"âœ… åµŒå…¥å‘é‡å·²åŠ è½½åˆ°æ–°çš„åµŒå…¥å±‚")
        
        # éªŒè¯åŠ è½½æ˜¯å¦æ­£ç¡®
        test_user_id = torch.tensor([100])
        original_vector = self.user_embedding(test_user_id)
        loaded_vector = new_embedding(test_user_id)
        
        is_same = torch.allclose(original_vector, loaded_vector)
        print(f"âœ… éªŒè¯åŠ è½½ç»“æœ: {'æˆåŠŸ' if is_same else 'å¤±è´¥'}")
        
        # å¯¼å‡ºä¸ºnumpyæ•°ç»„
        embedding_matrix = self.user_embedding.weight.data.numpy()
        np.save('user_embedding_matrix.npy', embedding_matrix)
        
        print(f"âœ… åµŒå…¥çŸ©é˜µå·²å¯¼å‡ºä¸ºnumpyæ•°ç»„")
        print(f"   æ–‡ä»¶å¤§å°: {embedding_matrix.nbytes / (1024*1024):.2f} MB")
        
        return embedding_matrix
    
    def visualization_example(self):
        """å¯è§†åŒ–ç¤ºä¾‹"""
        print(f"\nğŸ“Š åµŒå…¥å‘é‡å¯è§†åŒ–")
        print("=" * 60)
        
        # é€‰æ‹©ä¸€äº›ç”¨æˆ·è¿›è¡Œå¯è§†åŒ–
        user_ids = torch.tensor([0, 1, 2, 3, 4, 50, 100, 200, 500, 999])
        user_vectors = self.user_embedding(user_ids)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.cosine_similarity(
            user_vectors.unsqueeze(1), 
            user_vectors.unsqueeze(0), 
            dim=2
        )
        
        # ç»˜åˆ¶ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        plt.imshow(similarity_matrix.detach().numpy(), cmap='viridis', aspect='auto')
        plt.colorbar(label='ä½™å¼¦ç›¸ä¼¼åº¦')
        plt.title('ç”¨æˆ·åµŒå…¥å‘é‡ç›¸ä¼¼åº¦çŸ©é˜µ')
        plt.xlabel('ç”¨æˆ·ç´¢å¼•')
        plt.ylabel('ç”¨æˆ·ç´¢å¼•')
        
        # è®¾ç½®åæ ‡è½´æ ‡ç­¾
        user_labels = [f'User{uid.item()}' for uid in user_ids]
        plt.xticks(range(len(user_ids)), user_labels, rotation=45)
        plt.yticks(range(len(user_ids)), user_labels)
        
        plt.tight_layout()
        plt.savefig('user_similarity_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… ç›¸ä¼¼åº¦çƒ­åŠ›å›¾å·²ä¿å­˜ä¸º 'user_similarity_heatmap.png'")
        
        # æ˜¾ç¤ºåµŒå…¥å‘é‡çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nåµŒå…¥å‘é‡ç»Ÿè®¡ä¿¡æ¯ï¼š")
        all_embeddings = self.user_embedding.weight.data
        print(f"  å‘é‡èŒƒæ•°å‡å€¼: {torch.norm(all_embeddings, dim=1).mean():.4f}")
        print(f"  å‘é‡èŒƒæ•°æ ‡å‡†å·®: {torch.norm(all_embeddings, dim=1).std():.4f}")
        print(f"  å‘é‡å…ƒç´ å‡å€¼: {all_embeddings.mean():.6f}")
        print(f"  å‘é‡å…ƒç´ æ ‡å‡†å·®: {all_embeddings.std():.6f}")
        
        return similarity_matrix

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ PyTorchç”¨æˆ·IDåµŒå…¥å‘é‡ç”Ÿæˆç¤ºä¾‹")
    print("=" * 80)
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = UserEmbeddingDemo(num_users=1000, embedding_dim=64)
    
    # 1. åŸºç¡€åµŒå…¥å‘é‡ç”Ÿæˆ
    demo.basic_embedding_example()
    
    # 2. ç›¸ä¼¼åº¦è®¡ç®—
    demo.embedding_similarity_example()
    
    # 3. è®­ç»ƒè¿‡ç¨‹æ¼”ç¤º
    demo.training_example()
    
    # 4. å®é™…åº”ç”¨ç¤ºä¾‹
    demo.practical_application_example()
    
    # 5. ä¿å­˜å’ŒåŠ è½½
    demo.save_and_load_example()
    
    # 6. å¯è§†åŒ–
    demo.visualization_example()
    
    print(f"\n" + "=" * 80)
    print("ğŸ¯ å…³é”®ä»£ç æ€»ç»“:")
    print("""
    # 1. åˆ›å»ºåµŒå…¥å±‚
    user_embedding = nn.Embedding(num_users, embedding_dim)
    
    # 2. è·å–åµŒå…¥å‘é‡
    user_ids = torch.tensor([10, 20, 30])
    user_vectors = user_embedding(user_ids)
    
    # 3. åœ¨è®­ç»ƒä¸­ä½¿ç”¨
    optimizer = optim.Adam(user_embedding.parameters(), lr=0.01)
    loss.backward()
    optimizer.step()
    
    # 4. ä¿å­˜å’ŒåŠ è½½
    torch.save(user_embedding.state_dict(), 'embedding.pth')
    user_embedding.load_state_dict(torch.load('embedding.pth'))
    """)
    print("=" * 80)

if __name__ == "__main__":
    main() 